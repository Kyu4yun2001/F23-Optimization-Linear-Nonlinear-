{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "104da06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPrecompiling Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80]\n",
      "\n",
      "Please submit a bug report with steps to reproduce this fault, and any error messages that follow (in their entirety). Thanks.\n",
      "Exception: EXCEPTION_ACCESS_VIOLATION at 0x7b4e8b47 -- jl_svecref at C:/workdir/src\\julia.h:989 [inlined]\n",
      "jl_array_grow_at_end at C:/workdir/src\\array.c:880\n",
      "in expression starting at none:1\n",
      "jl_svecref at C:/workdir/src\\julia.h:989 [inlined]\n",
      "jl_array_grow_at_end at C:/workdir/src\\array.c:880\n",
      "ijl_array_grow_end at C:/workdir/src\\array.c:955\n",
      "_growend! at .\\array.jl:1014 [inlined]\n",
      "push! at .\\array.jl:1061 [inlined]\n",
      "load_libs at C:\\Users\\shimk\\.julia\\packages\\GR\\yBe3g\\src\\funcptrs.jl:54\n",
      "get_func_ptr at C:\\Users\\shimk\\.julia\\packages\\GR\\yBe3g\\src\\funcptrs.jl:75\n",
      "get_func_ptr at C:\\Users\\shimk\\.julia\\packages\\GR\\yBe3g\\src\\funcptrs.jl:75 [inlined]\n",
      "libGR_ptr at C:\\Users\\shimk\\.julia\\packages\\GR\\yBe3g\\src\\funcptrs.jl:84 [inlined]\n",
      "setcharheight at C:\\Users\\shimk\\.julia\\packages\\GR\\yBe3g\\src\\GR.jl:1552\n",
      "unknown function (ip: 01dfcaec)\n",
      "_jl_invoke at C:/workdir/src\\gf.c:2758 [inlined]\n",
      "ijl_apply_generic at C:/workdir/src\\gf.c:2940\n",
      "#gr_set_font#503 at C:\\Users\\shimk\\.julia\\packages\\Plots\\sxUvK\\src\\backends\\gr.jl:431\n",
      "gr_set_font at C:\\Users\\shimk\\.julia\\packages\\Plots\\sxUvK\\src\\backends\\gr.jl:422\n",
      "unknown function (ip: 01dfc713)\n",
      "_jl_invoke at C:/workdir/src\\gf.c:2758 [inlined]\n",
      "ijl_apply_generic at C:/workdir/src\\gf.c:2940\n",
      "#gr_set_tickfont#508 at C:\\Users\\shimk\\.julia\\packages\\Plots\\sxUvK\\src\\backends\\gr.jl:694\n",
      "gr_set_tickfont at C:\\Users\\shimk\\.julia\\packages\\Plots\\sxUvK\\src\\backends\\gr.jl:694\n",
      "unknown function (ip: 01df4e0c)\n",
      "_jl_invoke at C:/workdir/src\\gf.c:2758 [inlined]\n",
      "ijl_apply_generic at C:/workdir/src\\gf.c:2940\n",
      "_update_min_padding! at C:\\Users\\shimk\\.julia\\packages\\Plots\\sxUvK\\src\\backends\\gr.jl:870\n",
      "unknown function (ip: 1aae7d05)\n",
      "_jl_invoke at C:/workdir/src\\gf.c:2758 [inlined]\n",
      "ijl_apply_generic at C:/workdir/src\\gf.c:2940\n",
      "iterate at .\\generator.jl:47 [inlined]\n",
      "_collect at .\\array.jl:802\n",
      "collect_similar at .\\array.jl:711 [inlined]\n",
      "map at .\\abstractarray.jl:3263 [inlined]\n",
      "_update_min_padding! at C:\\Users\\shimk\\.julia\\packages\\Plots\\sxUvK\\src\\layouts.jl:277\n",
      "unknown function (ip: 1aae3205)\n",
      "_jl_invoke at C:/workdir/src\\gf.c:2758 [inlined]\n",
      "ijl_apply_generic at C:/workdir/src\\gf.c:2940\n",
      "prepare_output at C:\\Users\\shimk\\.julia\\packages\\Plots\\sxUvK\\src\\plot.jl:239\n",
      "show at C:\\Users\\shimk\\.julia\\packages\\Plots\\sxUvK\\src\\output.jl:231 [inlined]\n",
      "#344 at C:\\Users\\shimk\\.julia\\packages\\Plots\\sxUvK\\src\\output.jl:6 [inlined]\n",
      "#open#409 at .\\io.jl:395\n",
      "open at .\\io.jl:392 [inlined]\n",
      "png at C:\\Users\\shimk\\.julia\\packages\\Plots\\sxUvK\\src\\output.jl:6\n",
      "unknown function (ip: 1aae10dc)\n",
      "_jl_invoke at C:/workdir/src\\gf.c:2758 [inlined]\n",
      "ijl_apply_generic at C:/workdir/src\\gf.c:2940\n",
      "savefig at C:\\Users\\shimk\\.julia\\packages\\Plots\\sxUvK\\src\\output.jl:149\n",
      "unknown function (ip: 1aae051c)\n",
      "_jl_invoke at C:/workdir/src\\gf.c:2758 [inlined]\n",
      "ijl_apply_generic at C:/workdir/src\\gf.c:2940\n",
      "##1#293 at C:\\Users\\shimk\\.julia\\packages\\Plots\\sxUvK\\src\\init.jl:111\n",
      "unknown function (ip: 1aa8628d)\n",
      "_jl_invoke at C:/workdir/src\\gf.c:2758 [inlined]\n",
      "ijl_apply_generic at C:/workdir/src\\gf.c:2940\n",
      "jl_apply at C:/workdir/src\\julia.h:1880 [inlined]\n",
      "do_call at C:/workdir/src\\interpreter.c:126\n",
      "eval_value at C:/workdir/src\\interpreter.c:226\n",
      "eval_stmt_value at C:/workdir/src\\interpreter.c:177 [inlined]\n",
      "eval_body at C:/workdir/src\\interpreter.c:606\n",
      "jl_interpret_toplevel_thunk at C:/workdir/src\\interpreter.c:762\n",
      "jl_toplevel_eval_flex at C:/workdir/src\\toplevel.c:912\n",
      "ijl_toplevel_eval at C:/workdir/src\\toplevel.c:921 [inlined]\n",
      "ijl_toplevel_eval_in at C:/workdir/src\\toplevel.c:971\n",
      "eval at .\\boot.jl:370 [inlined]\n",
      "eval at C:\\Users\\shimk\\.julia\\packages\\Plots\\sxUvK\\src\\Plots.jl:1 [inlined]\n",
      "_broadcast_getindex_evalf at .\\broadcast.jl:683 [inlined]\n",
      "_broadcast_getindex at .\\broadcast.jl:656 [inlined]\n",
      "getindex at .\\broadcast.jl:610 [inlined]\n",
      "copy at .\\broadcast.jl:912 [inlined]\n",
      "materialize at .\\broadcast.jl:873\n",
      "unknown function (ip: 1aa76705)\n",
      "_jl_invoke at C:/workdir/src\\gf.c:2739 [inlined]\n",
      "ijl_apply_generic at C:/workdir/src\\gf.c:2940\n",
      "macro expansion at C:\\Users\\shimk\\.julia\\packages\\Plots\\sxUvK\\src\\init.jl:126 [inlined]\n",
      "macro expansion at C:\\Users\\shimk\\.julia\\packages\\PrecompileTools\\kmH5L\\src\\workloads.jl:78 [inlined]\n",
      "#557 at C:\\Users\\shimk\\.julia\\packages\\Plots\\sxUvK\\src\\init.jl:123\n",
      "withenv at .\\env.jl:197\n",
      "unknown function (ip: 1aa75bc7)\n",
      "_jl_invoke at C:/workdir/src\\gf.c:2758 [inlined]\n",
      "ijl_apply_generic at C:/workdir/src\\gf.c:2940\n",
      "macro expansion at C:\\Users\\shimk\\.julia\\packages\\Plots\\sxUvK\\src\\init.jl:122 [inlined]\n",
      "top-level scope at C:\\Users\\shimk\\.julia\\packages\\PrecompileTools\\kmH5L\\src\\workloads.jl:140\n",
      "jl_fptr_args at C:/workdir/src\\gf.c:2405\n",
      "_jl_invoke at C:/workdir/src\\gf.c:2758 [inlined]\n",
      "ijl_invoke at C:/workdir/src\\gf.c:2765\n",
      "jl_toplevel_eval_flex at C:/workdir/src\\toplevel.c:903\n",
      "jl_toplevel_eval_flex at C:/workdir/src\\toplevel.c:856\n",
      "ijl_toplevel_eval at C:/workdir/src\\toplevel.c:921 [inlined]\n",
      "ijl_toplevel_eval_in at C:/workdir/src\\toplevel.c:971\n",
      "eval at .\\boot.jl:370 [inlined]\n",
      "include_string at .\\loading.jl:1903\n",
      "jl_fptr_args at C:/workdir/src\\gf.c:2405\n",
      "_jl_invoke at C:/workdir/src\\gf.c:2739 [inlined]\n",
      "ijl_apply_generic at C:/workdir/src\\gf.c:2940\n",
      "_include at .\\loading.jl:1963\n",
      "include at .\\Base.jl:457\n",
      "jfptr_include_39355 at C:\\Users\\shimk\\AppData\\Local\\Programs\\Julia-1.9.3\\lib\\julia\\sys.dll (unknown line)\n",
      "_jl_invoke at C:/workdir/src\\gf.c:2739 [inlined]\n",
      "ijl_apply_generic at C:/workdir/src\\gf.c:2940\n",
      "jl_apply at C:/workdir/src\\julia.h:1880 [inlined]\n",
      "jl_f__call_latest at C:/workdir/src\\builtins.c:774\n",
      "include at C:\\Users\\shimk\\.julia\\packages\\Plots\\sxUvK\\src\\Plots.jl:1\n",
      "unknown function (ip: 1a9f4235)\n",
      "_jl_invoke at C:/workdir/src\\gf.c:2739 [inlined]\n",
      "ijl_apply_generic at C:/workdir/src\\gf.c:2940\n",
      "jl_apply at C:/workdir/src\\julia.h:1880 [inlined]\n",
      "do_call at C:/workdir/src\\interpreter.c:126\n",
      "eval_value at C:/workdir/src\\interpreter.c:226\n",
      "eval_stmt_value at C:/workdir/src\\interpreter.c:177 [inlined]\n",
      "eval_body at C:/workdir/src\\interpreter.c:606\n",
      "jl_interpret_toplevel_thunk at C:/workdir/src\\interpreter.c:762\n",
      "jl_toplevel_eval_flex at C:/workdir/src\\toplevel.c:912\n",
      "jl_eval_module_expr at C:/workdir/src\\toplevel.c:203 [inlined]\n",
      "jl_toplevel_eval_flex at C:/workdir/src\\toplevel.c:715\n",
      "jl_toplevel_eval_flex at C:/workdir/src\\toplevel.c:856\n",
      "ijl_toplevel_eval at C:/workdir/src\\toplevel.c:921 [inlined]\n",
      "ijl_toplevel_eval_in at C:/workdir/src\\toplevel.c:971\n",
      "eval at .\\boot.jl:370 [inlined]\n",
      "include_string at .\\loading.jl:1903\n",
      "jl_fptr_args at C:/workdir/src\\gf.c:2405\n",
      "_jl_invoke at C:/workdir/src\\gf.c:2739 [inlined]\n",
      "ijl_apply_generic at C:/workdir/src\\gf.c:2940\n",
      "_include at .\\loading.jl:1963\n",
      "include at .\\Base.jl:457 [inlined]\n",
      "include_package_for_output at .\\loading.jl:2049\n",
      "jfptr_include_package_for_output_40754 at C:\\Users\\shimk\\AppData\\Local\\Programs\\Julia-1.9.3\\lib\\julia\\sys.dll (unknown line)\n",
      "_jl_invoke at C:/workdir/src\\gf.c:2739 [inlined]\n",
      "ijl_apply_generic at C:/workdir/src\\gf.c:2940\n",
      "jl_apply at C:/workdir/src\\julia.h:1880 [inlined]\n",
      "do_call at C:/workdir/src\\interpreter.c:126\n",
      "eval_value at C:/workdir/src\\interpreter.c:226\n",
      "eval_stmt_value at C:/workdir/src\\interpreter.c:177 [inlined]\n",
      "eval_body at C:/workdir/src\\interpreter.c:606\n",
      "jl_interpret_toplevel_thunk at C:/workdir/src\\interpreter.c:762\n",
      "jl_toplevel_eval_flex at C:/workdir/src\\toplevel.c:912\n",
      "jl_toplevel_eval_flex at C:/workdir/src\\toplevel.c:856\n",
      "ijl_toplevel_eval at C:/workdir/src\\toplevel.c:921 [inlined]\n",
      "ijl_toplevel_eval_in at C:/workdir/src\\toplevel.c:971\n",
      "eval at .\\boot.jl:370 [inlined]\n",
      "include_string at .\\loading.jl:1903\n",
      "include_string at .\\loading.jl:1913 [inlined]\n",
      "exec_options at .\\client.jl:305\n",
      "_start at .\\client.jl:522\n",
      "jfptr__start_45166 at C:\\Users\\shimk\\AppData\\Local\\Programs\\Julia-1.9.3\\lib\\julia\\sys.dll (unknown line)\n",
      "_jl_invoke at C:/workdir/src\\gf.c:2739 [inlined]\n",
      "ijl_apply_generic at C:/workdir/src\\gf.c:2940\n",
      "jl_apply at C:/workdir/src\\julia.h:1880 [inlined]\n",
      "true_main at C:/workdir/src\\jlapi.c:573\n",
      "jl_repl_entrypoint at C:/workdir/src\\jlapi.c:717\n",
      "jl_load_repl at C:/workdir/cli\\loader_lib.c:529\n",
      "mainCRTStartup at C:/workdir/cli\\loader_exe.c:59\n",
      "BaseThreadInitThunk at C:\\WINDOWS\\System32\\KERNEL32.DLL (unknown line)\n",
      "RtlInitializeExceptionChain at C:\\WINDOWS\\SYSTEM32\\ntdll.dll (unknown line)\n",
      "RtlClearBits at C:\\WINDOWS\\SYSTEM32\\ntdll.dll (unknown line)\n",
      "Allocations: 29263015 (Pool: 29256077; Big: 6938); GC: 88\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "Failed to precompile Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80] to \"C:\\\\Users\\\\shimk\\\\.julia\\\\compiled\\\\v1.9\\\\Plots\\\\jl_93A8.tmp\".",
     "output_type": "error",
     "traceback": [
      "Failed to precompile Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80] to \"C:\\\\Users\\\\shimk\\\\.julia\\\\compiled\\\\v1.9\\\\Plots\\\\jl_93A8.tmp\".",
      "",
      "Stacktrace:",
      " [1] error(s::String)",
      "   @ Base .\\error.jl:35",
      " [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, keep_loaded_modules::Bool)",
      "   @ Base .\\loading.jl:2300",
      " [3] compilecache",
      "   @ .\\loading.jl:2167 [inlined]",
      " [4] _require(pkg::Base.PkgId, env::String)",
      "   @ Base .\\loading.jl:1805",
      " [5] _require_prelocked(uuidkey::Base.PkgId, env::String)",
      "   @ Base .\\loading.jl:1660",
      " [6] macro expansion",
      "   @ .\\loading.jl:1648 [inlined]",
      " [7] macro expansion",
      "   @ .\\lock.jl:267 [inlined]",
      " [8] require(into::Module, mod::Symbol)",
      "   @ Base .\\loading.jl:1611"
     ]
    }
   ],
   "source": [
    "# Implementation of Newton's method, Steepest Descent (constant step size, & Armijo's rule), and Momentum Descent\n",
    "# This version: for HW5\n",
    "using Printf\n",
    "using LinearAlgebra\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8ee6cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "include(\"data-hw5.jl\");\n",
    "z = data[:,1];\n",
    "y = data[:,2];\n",
    "n = length(z);   # number of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa56257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters here, for all optimization algorithms\n",
    "tol = 1e-6;     # tolerance on norm of gradient\n",
    "MaxIter = 1e5;  # maximum number of iterations of gradient descent\n",
    "MaxIterNewton = 500;  # maximum number of iterations for Newton-type methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f3185a",
   "metadata": {},
   "source": [
    "### Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30f62dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dm2 (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sigmoidal function, and its derivatives\n",
    "function s(x)\n",
    "   return 1/(1+exp(-x));\n",
    "end\n",
    "\n",
    "function sp(x)\n",
    "    return exp(-x)/(1+exp(-x))^2;\n",
    "end\n",
    "\n",
    "function spp(x)\n",
    "    return 2*exp(-2*x)/(1+exp(-x))^3 - exp(-x)/(1+exp(-x))^2;\n",
    "end\n",
    "\n",
    "# model function: y = m(z,x), where x are parameters\n",
    "function m(z,x)\n",
    "    a = x[1];\n",
    "    b = x[2];\n",
    "    return s(a*z+b);\n",
    "end\n",
    "\n",
    "# gradient of model function\n",
    "function Dm(z,x)\n",
    "   a = x[1]; b = x[2]; \n",
    "   g1 = z*sp(a*z+b);\n",
    "   g2 = sp(a*z + b); # Inserted\n",
    "   return [g1;g2];\n",
    "end\n",
    "\n",
    "# Hessian of model function\n",
    "function Dm2(z,x)\n",
    "   a = x[1]; b = x[2]; \n",
    "   H = zeros(2,2);\n",
    "   ### YOU INSERT ###\n",
    "   H[1,1] = z*z*spp(a*z + b);\n",
    "   H[1,2] = z*spp(a*z+b);\n",
    "   H[2,1] = z*spp(a*z+b);\n",
    "   H[2,2] = spp(a*z+b);\n",
    "   ### END INSERT ###\n",
    "   return H;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "75005bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DF2 (generic function with 1 method)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss function\n",
    "function F(x)\n",
    "   L = 0;\n",
    "   for i=1:n\n",
    "      L = L + 0.5*(y[i] - m(z[i],x))^2;\n",
    "   end\n",
    "   return L;\n",
    "end\n",
    "\n",
    "# gradient of Loss function\n",
    "function DF(x)\n",
    "   g = zeros(2);\n",
    "   for i=1:n\n",
    "      g = g-(y[i]-m(z[i],x))*Dm(z[i],x);\n",
    "   end\n",
    "   return g;\n",
    "end\n",
    "\n",
    "# Hessian of Loss function\n",
    "function DF2(x)\n",
    "    H = zeros(2,2);\n",
    "    for i=1:n\n",
    "      H = H - ( (y[i]-m(z[i],x))*Dm2(z[i],x) - Dm(z[i],x)*Dm(z[i],x)');\n",
    "   end\n",
    "   return H;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f201649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SteepestDescent (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# steepest descent algorithm, with constant step size\n",
    "# input: \n",
    "#    x0 = initial point\n",
    "#    alpha = step size. Constant, in this algorithm.\n",
    "\n",
    "# output: \n",
    "#    xsave = list of points\n",
    "\n",
    "function SteepestDescent(x0,alpha)\n",
    "\n",
    "   # setup for steepest descent\n",
    "   x = x0;\n",
    "   successflag = false;\n",
    "   xsave = zeros(length(x0),MaxIter+1);\n",
    "   xsave[:,1] = x0;\n",
    "\n",
    "   # perform steepest descent iterations\n",
    "   for iter = 1:MaxIter\n",
    "\n",
    "       # compute gradient\n",
    "       Fgrad = DF(x);\n",
    "\n",
    "       # print info\n",
    "       # @printf(\"x = %11.10f, %11.10f, F(x) = %10.8f, |grad F| = %10.8f \\n\", x[1],x[2],F(x),sqrt(Fgrad'*Fgrad));\n",
    "\n",
    "       # check if gradient is small enough\n",
    "       if sqrt(Fgrad'*Fgrad) < tol\n",
    "          @printf(\"x = %11.10f, %11.10f, F(x) = %10.8f, |grad F| = %10.8f \\n\", x[1],x[2],F(x),sqrt(Fgrad'*Fgrad));\n",
    "            @printf(\"Converged after %d iterations, function value %f\\n\", iter, F(x))\n",
    "          successflag = true;\n",
    "          xsave = xsave[:,1:iter];\n",
    "          break;\n",
    "       end\n",
    "\n",
    "       # perform steepest descent step\n",
    "       x = x - alpha*Fgrad;\n",
    "       \n",
    "       # save point\n",
    "       xsave[:,iter+1] = x;\n",
    "   end\n",
    "   if successflag == false\n",
    "       @printf(\"Failed to converge after %d iterations, function value %f\\n\", MaxIter, F(x));\n",
    "   end\n",
    "   return xsave;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53c89d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SteepestDescentArmijo (generic function with 1 method)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# steepest descent algorithm, with Armijo's rule for backtracking\n",
    "# input: \n",
    "#    x0 = initial point\n",
    "#    c1 = slope, in Armijo's rule.\n",
    "# output: \n",
    "#    xsave = list of points\n",
    "#\n",
    "function SteepestDescentArmijo(x0)\n",
    "\n",
    "   # parameters\n",
    "   alpha0 = 10.0;    # initial value of alpha, to try in backtracking\n",
    "   eta = 0.5;       # factor with which to scale alpha, each time you backtrack\n",
    "   MaxBacktrack = 20;  # maximum number of backtracking steps\n",
    "   c1 = 1e-2;       # slope factor, in Armijo's rule\n",
    "\n",
    "   # setup for steepest descent\n",
    "   x = x0;\n",
    "   successflag = false;   \n",
    "   xsave = zeros(length(x0),MaxIter);\n",
    "   xsave[:,1] = x0;\n",
    "\n",
    "   # perform steepest descent iterations\n",
    "   for iter = 1:MaxIter\n",
    "\n",
    "      alpha = alpha0;\n",
    "\n",
    "      # compute gradient\n",
    "       Fgrad = DF(x);\n",
    "\n",
    "       # print info\n",
    "       # @printf(\"x = %11.10f, %11.10f, F(x) = %10.8f, |grad F| = %10.8f \\n\", x[1],x[2],F(x),sqrt(Fgrad'*Fgrad));\n",
    "\n",
    "      # check if norm of gradient is small enough\n",
    "      if sqrt(Fgrad'*Fgrad) < tol\n",
    "        @printf(\"x = %11.10f, %11.10f, F(x) = %10.8f, |grad F| = %10.8f \\n\", x[1],x[2],F(x),sqrt(Fgrad'*Fgrad));\n",
    "         @printf(\"Converged after %d iterations, function value %f\\n\", iter, F(x))\n",
    "         successflag = true;\n",
    "         xsave = xsave[:,1:iter];\n",
    "         break;\n",
    "      end\n",
    "\n",
    "      # perform line search\n",
    "      Fval = F(x);\n",
    "      for k = 1:MaxBacktrack\n",
    "         x_try = x - alpha*Fgrad;\n",
    "         Fval_try = F(x_try);\n",
    "         if (Fval_try > Fval - c1*alpha *Fgrad'Fgrad)\n",
    "            alpha = alpha * eta;\n",
    "         else\n",
    "            Fval = Fval_try;\n",
    "            x = x_try;\n",
    "            break;\n",
    "         end\n",
    "      end\n",
    "\n",
    "      # save point\n",
    "      xsave[:,iter+1] = x;\n",
    "\n",
    "      # print how we're doing, every 10 iterations\n",
    "      #if (iter%5==0)\n",
    "      #   @printf(\"iter: %d: alpha: %f, %f, %f, %f\\n\", iter, alpha, x[1], x[2], F(x))\n",
    "      #end \n",
    "   end\n",
    "\n",
    "   if successflag == false\n",
    "       @printf(\"Failed to converge after %d iterations, function value %f\\n\", MaxIter, F(x))\n",
    "   end\n",
    "\n",
    "   return xsave\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "086e4cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Newtonv0 (generic function with 1 method)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Newton's algorithm\n",
    "# input: \n",
    "#    x0 = initial point, a 2-vector (e.g. x0=[1;2])\n",
    "# output: \n",
    "#    xsave = list of points\n",
    "\n",
    "function Newtonv0(x0) # Original Version\n",
    "\n",
    "   # setup \n",
    "   x = x0;\n",
    "   successflag = false;\n",
    "   xsave = zeros(length(x0),MaxIter+1);\n",
    "   xsave[:,1] = x0;\n",
    "\n",
    "   # iterate\n",
    "   for iter = 1:MaxIterNewton\n",
    "      \n",
    "       # compute gradient\n",
    "       Fgrad = DF(x);\n",
    "\n",
    "       #@printf(\"x = %11.10f, %11.10f, F(x) = %10.8f, |grad F| = %10.8f \\n\", x[1],x[2],F(x),sqrt(Fgrad'*Fgrad));\n",
    "\n",
    "       # check whether gradient is small enough\n",
    "       if sqrt(Fgrad'*Fgrad) < tol\n",
    "            @printf(\"x = %11.10f, %11.10f, F(x) = %10.8f, |grad F| = %10.8f \\n\", x[1],x[2],F(x),sqrt(Fgrad'*Fgrad));\n",
    "          @printf(\"\\nConverged after %d iterations, F(x) = %f\\n\", iter, F(x));\n",
    "          println(\"x = \", x');\n",
    "          successflag = true;\n",
    "          xsave = xsave[:,1:iter];\n",
    "          break;\n",
    "       end\n",
    "\n",
    "       # compute Hessian\n",
    "       H = DF2(x);\n",
    "\n",
    "       # check if hessian is positive definite\n",
    "       ### YOU INSERT ###\n",
    "        \n",
    "\n",
    "       # Newton step\n",
    "       x = x - inv(H) * Fgrad;    # normally you don't actually compute a matrix inverse\n",
    "       \n",
    "       # save point\n",
    "       xsave[:,iter+1] = x;\n",
    "   end\n",
    "    \n",
    "   if successflag == false\n",
    "       @printf(\"Failed to converge after %d iterations, function value %F\\n\", MaxIter, F(x))\n",
    "   end\n",
    "    \n",
    "   return xsave;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "3bd1a515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Newtonv1 (generic function with 1 method)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Newton's algorithm\n",
    "# input: \n",
    "#    x0 = initial point, a 2-vector (e.g. x0=[1;2])\n",
    "# output: \n",
    "#    xsave = list of points\n",
    "\n",
    "function Newtonv1(x0)\n",
    "    # setup \n",
    "    x = x0\n",
    "    successflag = false\n",
    "    xsave = zeros(length(x0), MaxIter + 1)\n",
    "    xsave[:, 1] = x0\n",
    "\n",
    "    # iterate\n",
    "    for iter = 1:MaxIterNewton\n",
    "        try\n",
    "            # compute gradient\n",
    "            Fgrad = DF(x)\n",
    "\n",
    "            #@printf(\"x = %11.10f, %11.10f, F(x) = %10.8f, |grad F| = %10.8f \\n\", x[1],x[2],F(x),sqrt(Fgrad'*Fgrad));\n",
    "\n",
    "            # check whether gradient is small enough\n",
    "            if sqrt(Fgrad' * Fgrad) < tol\n",
    "                @printf(\"\\nConverged after %d iterations, F(x) = %f\\n\", iter, F(x))\n",
    "                println(\"x = \", x')\n",
    "                successflag = true\n",
    "                xsave = xsave[:, 1:iter]\n",
    "                break\n",
    "            end\n",
    "\n",
    "            # compute Hessian\n",
    "            H = DF2(x)\n",
    "\n",
    "            # Newton step\n",
    "            x = x - inv(H) * Fgrad  # normally you don't actually compute a matrix inverse\n",
    "\n",
    "            # save point\n",
    "            xsave[:, iter + 1] = x\n",
    "        catch e\n",
    "            println(\"Caught an error: \", e)\n",
    "            println(\"Skipping iteration \", iter)\n",
    "            @printf(\"Failed to converge after %d iterations, function value %F\\n\", iter, F(x))\n",
    "            break  # Break out of the loop if an error occurs\n",
    "            \n",
    "        end\n",
    "    end\n",
    "\n",
    "    if successflag == false\n",
    "        @printf(\"Failed to converge after %d iterations, function value %F\\n\", MaxIter, F(x))\n",
    "    end\n",
    "\n",
    "    return xsave\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a66a506c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Newtonv2 (generic function with 1 method)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Newton's algorithm\n",
    "# input: \n",
    "#    x0 = initial point, a 2-vector (e.g. x0=[1;2])\n",
    "# output: \n",
    "#    xsave = list of points\n",
    "\n",
    "function Newtonv2(x0)\n",
    "    # setup \n",
    "    x = x0\n",
    "    successflag = false\n",
    "    xsave = zeros(length(x0), MaxIter + 1)\n",
    "    xsave[:, 1] = x0\n",
    "\n",
    "    # iterate\n",
    "    for iter = 1:MaxIterNewton\n",
    "        try\n",
    "            # compute gradient\n",
    "            Fgrad = DF(x)\n",
    "\n",
    "            #@printf(\"x = %11.10f, %11.10f, F(x) = %10.8f, |grad F| = %10.8f \\n\", x[1],x[2],F(x),sqrt(Fgrad'*Fgrad));\n",
    "\n",
    "            # check whether gradient is small enough\n",
    "            if sqrt(Fgrad' * Fgrad) < tol\n",
    "                @printf(\"\\nConverged after %d iterations, F(x) = %f\\n\", iter, F(x))\n",
    "                println(\"x = \", x')\n",
    "                successflag = true\n",
    "                xsave = xsave[:, 1:iter]\n",
    "                break\n",
    "            end\n",
    "\n",
    "            # compute Hessian\n",
    "            H = DF2(x)\n",
    "\n",
    "            # check if hessian is positive definite\n",
    "            min_eval = findmin(eigen(H).values)[1]\n",
    "            print(\"\\n Minimum eigenvalue: \", min_eval, \"\\n\")\n",
    "\n",
    "            # Newton step\n",
    "            x = x - inv(H) * Fgrad  # normally you don't actually compute a matrix inverse\n",
    "\n",
    "            # save point\n",
    "            xsave[:, iter + 1] = x\n",
    "        catch e\n",
    "            println(\"Caught an error: \", e)\n",
    "            println(\"Skipping iteration \", iter)\n",
    "            @printf(\"Failed to converge after %d iterations, function value %F\\n\", iter, F(x))\n",
    "            break  # Break out of the loop if an error occurs\n",
    "            \n",
    "        end\n",
    "    end\n",
    "\n",
    "    if successflag == false\n",
    "        @printf(\"Failed to converge after %d iterations, function value %F\\n\", MaxIter, F(x))\n",
    "    end\n",
    "\n",
    "    return xsave\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "42bbc085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NewtonModified (generic function with 1 method)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Newton's algorithm, with Hessian modification\n",
    "# input: \n",
    "#    x0 = initial point, a 2-vector (e.g. x0=[1;2])\n",
    "# output: \n",
    "#    xsave = list of points\n",
    "#\n",
    "function NewtonModified(x0)\n",
    "\n",
    "   # parameters\n",
    "   backtrack = false;  # whether or not to apply backtracking\n",
    "   eta = 0.5;       # factor with which to scale alpha, each time you backtrack\n",
    "   MaxBacktrack = 20;  # maximum number of backtracking steps\n",
    "   c1 = 1e-3;       # slope factor, in Armijo's rule\n",
    "\n",
    "\n",
    "   # setup \n",
    "   x = x0;\n",
    "   successflag = false;\n",
    "   xsave = zeros(length(x0),MaxIter+1);\n",
    "   xsave[:,1] = x0;\n",
    "\n",
    "   # iterate\n",
    "   for iter = 1:MaxIterNewton\n",
    "        try\n",
    "            # compute gradient\n",
    "           Fgrad = DF(x);\n",
    "            \n",
    "\n",
    "           # check whether gradient is small enough\n",
    "           if sqrt(Fgrad'*Fgrad) < tol\n",
    "              @printf(\"\\nConverged after %d iterations, F(x) = %f\\n\", iter, F(x));\n",
    "              println(\"\\n x = \", x');\n",
    "              successflag = true;\n",
    "              xsave = xsave[:,1:iter];\n",
    "              break;\n",
    "           end\n",
    "\n",
    "           # Find multiple of identity to add to Hessian, to make it positive definite\n",
    "           H = DF2(x);   # Hessian\n",
    "            tau = 0;\n",
    "            delta = 2;\n",
    "\n",
    "           ### YOU INSERT ### \n",
    "            B = H + tau*I(size(H)[1]);    # YOU MUST POSSIBLY MODIFY B HERE\n",
    "\n",
    "            min_B_eval = findmin(eigen(B).values)[1]\n",
    "            if min_B_eval < delta\n",
    "                tau = max(tau, (delta - min_B_eval) + 0.01)\n",
    "                # print(\"\\n min_B_eval:\", min_B_eval, \", tau: \", tau, \"\\n\")\n",
    "            end\n",
    "\n",
    "            ### END INSERT ### \n",
    "\n",
    "           d = inv(B)*Fgrad;  # descent direction\n",
    "           alpha = 1;\n",
    "\n",
    "           # find step size alpha, using backtracking\n",
    "           if backtrack\n",
    "              Fval = F(x);\n",
    "              for k = 1:MaxBacktrack\n",
    "                x_try = x - alpha*d;\n",
    "                Fval_try = F(x_try);\n",
    "                if (Fval_try > Fval - c1*alpha *Fgrad'*d)\n",
    "                   alpha = alpha * eta;\n",
    "                else\n",
    "                   break;\n",
    "                end\n",
    "             end\n",
    "          end\n",
    "\n",
    "          @printf(\"x = %11.10f, %11.10f, F(x) = %10.8f, |grad F| = %10.8f, tau = %6.4f, alpha = %6.4f \\n\", x[1],x[2],F(x),sqrt(Fgrad'*Fgrad),tau, alpha);\n",
    "           # take step\n",
    "           x = x - alpha*d;    # normally you don't actually compute a matrix inverse\n",
    "\n",
    "           # save point\n",
    "           xsave[:,iter+1] = x;\n",
    "       \n",
    "        catch e\n",
    "            println(\"Caught an error: \", e)\n",
    "            println(\"Skipping iteration \", iter)\n",
    "            @printf(\"Failed to converge after %d iterations, function value %F\\n\", iter, F(x))\n",
    "            break  # Break out of the loop if an error occurs\n",
    "        end\n",
    "   end\n",
    "   if successflag == false\n",
    "       @printf(\"Failed to converge after %d iterations, function value %F\\n\", MaxIter, F(x))\n",
    "   end\n",
    "   return xsave;\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "99833531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NewtonModified_Backtrack (generic function with 1 method)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Newton's algorithm, with Hessian modification\n",
    "# input: \n",
    "#    x0 = initial point, a 2-vector (e.g. x0=[1;2])\n",
    "# output: \n",
    "#    xsave = list of points\n",
    "#\n",
    "function NewtonModified_Backtrack(x0)\n",
    "\n",
    "   # parameters\n",
    "   backtrack = true;  # whether or not to apply backtracking\n",
    "   eta = 0.5;       # factor with which to scale alpha, each time you backtrack\n",
    "   MaxBacktrack = 20;  # maximum number of backtracking steps\n",
    "   c1 = 1e-3;       # slope factor, in Armijo's rule\n",
    "\n",
    "\n",
    "   # setup \n",
    "   x = x0;\n",
    "   successflag = false;\n",
    "   xsave = zeros(length(x0),MaxIter+1);\n",
    "   xsave[:,1] = x0;\n",
    "\n",
    "   # iterate\n",
    "   for iter = 1:MaxIterNewton\n",
    "        try\n",
    "            # compute gradient\n",
    "           Fgrad = DF(x);\n",
    "            \n",
    "\n",
    "           # check whether gradient is small enough\n",
    "           if sqrt(Fgrad'*Fgrad) < tol\n",
    "              @printf(\"\\nConverged after %d iterations, F(x) = %f\\n\", iter, F(x));\n",
    "              println(\"\\n x = \", x');\n",
    "              successflag = true;\n",
    "              xsave = xsave[:,1:iter];\n",
    "              break;\n",
    "           end\n",
    "\n",
    "           # Find multiple of identity to add to Hessian, to make it positive definite\n",
    "           H = DF2(x);   # Hessian\n",
    "            tau = 0;\n",
    "            delta = 2;\n",
    "\n",
    "           ### YOU INSERT ### \n",
    "            B = H + tau*I(size(H)[1]);    # YOU MUST POSSIBLY MODIFY B HERE\n",
    "\n",
    "            min_B_eval = findmin(eigen(B).values)[1]\n",
    "            if min_B_eval < delta\n",
    "                tau = max(tau, (delta - min_B_eval) + 0.01)\n",
    "                # print(\"\\n min_B_eval:\", min_B_eval, \", tau: \", tau, \"\\n\")\n",
    "            end\n",
    "\n",
    "            ### END INSERT ### \n",
    "\n",
    "           d = inv(B)*Fgrad;  # descent direction\n",
    "           alpha = 1;\n",
    "\n",
    "           # find step size alpha, using backtracking\n",
    "           if backtrack\n",
    "              Fval = F(x);\n",
    "              for k = 1:MaxBacktrack\n",
    "                x_try = x - alpha*d;\n",
    "                Fval_try = F(x_try);\n",
    "                if (Fval_try > Fval - c1*alpha *Fgrad'*d)\n",
    "                   alpha = alpha * eta;\n",
    "                else\n",
    "                   break;\n",
    "                end\n",
    "             end\n",
    "          end\n",
    "\n",
    "          # @printf(\"x = %11.10f, %11.10f, F(x) = %10.8f, |grad F| = %10.8f, tau = %6.4f, alpha = %6.4f \\n\", x[1],x[2],F(x),sqrt(Fgrad'*Fgrad),tau, alpha);\n",
    "           # take step\n",
    "           x = x - alpha*d;    # normally you don't actually compute a matrix inverse\n",
    "\n",
    "           # save point\n",
    "           xsave[:,iter+1] = x;\n",
    "       \n",
    "        catch e\n",
    "            println(\"Caught an error: \", e)\n",
    "            println(\"Skipping iteration \", iter)\n",
    "            @printf(\"Failed to converge after %d iterations, function value %F\\n\", iter, F(x))\n",
    "            break  # Break out of the loop if an error occurs\n",
    "        end\n",
    "   end\n",
    "   if successflag == false\n",
    "       @printf(\"Failed to converge after %d iterations, function value %F\\n\", MaxIter, F(x))\n",
    "   end\n",
    "   return xsave;\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1e69a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "plotcontours (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot contours of function\n",
    "function plotcontours(xl,yl)\n",
    "   xmin = -xl;\n",
    "   xmax = xl;\n",
    "   ymin = -yl;\n",
    "   ymax = yl;\n",
    "   x = xmin:0.05:xmax;\n",
    "   y = ymin:0.05:ymax;\n",
    "   Z = zeros(length(y), length(x));\n",
    "   for i=1:length(x)\n",
    "      for j=1:length(y)\n",
    "        Z[j,i] = F([x[i]; y[j]]);\n",
    "      end\n",
    "   end\n",
    "   \n",
    "   contourf(x,y,Z,levels=20,aspect_ratio=:equal,fill=(true,cgrad(:inferno,rev=true)));\n",
    "   #contourf(x,y,Z,levels=30,fill=(true,cgrad(:haline ,[0,0.1,1.0])));\n",
    "   #contourf(x,y,Z,levels=30,fill=(true,cgrad(:deep,[0,0.2,1.0])));\n",
    "   #contourf(x,y,Z,levels=30,fill=(true,cgrad(:inferno,scale=:exp)));\n",
    "\n",
    "   # color schemes, see:\n",
    "   #   http://docs.juliaplots.org/latest/generated/colorschemes/#cmocean \n",
    "   # see also\n",
    "   #   https://github.com/JuliaPlots/ExamplePlots.jl/blob/master/notebooks/cgrad.ipynb\n",
    "   #   https://docs.juliaplots.org/latest/generated/colorschemes/\n",
    "   #   https://github.com/JuliaGraphics/Colors.jl/blob/master/src/names_data.jl\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e462e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "plotdata (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot data on top of contour plot\n",
    "function plotdata(xsave,str)\n",
    "   K= length(xsave[1,:]);\n",
    "   println(\"k = \", K)\n",
    "   plot!(xsave[1,1:1:K], xsave[2,1:1:K], lw = 2, marker=2, label = str,legend=:bottomright);\n",
    "   #savefig(\"plot.png\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0be7dae",
   "metadata": {},
   "source": [
    "### Code Implementation for Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "486aec06",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Matrix{Float64}:\n",
       " -0.31985   -0.159925\n",
       " -0.159925  -0.0799625"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q5A - testing Dm(z, x), Dm^2(z,x)\n",
    "Dm(2, [1,0])\n",
    "Dm2(2, [1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7f51a26",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1 alpha: 0.05 starting points:[-0.5021926059012495, -0.5342527158461803]\n",
      "x = 0.7011309324, -3.4622019552, F(x) = 0.31707947, |grad F| = 0.00000100 \n",
      "Converged after 5591 iterations, function value 0.317079\n",
      "\n",
      "iter: 1 alpha: 0.1 starting points:[-0.5021926059012495, -0.5342527158461803]\n",
      "x = 0.7011309403, -3.4622019964, F(x) = 0.31707947, |grad F| = 0.00000100 \n",
      "Converged after 2778 iterations, function value 0.317079\n",
      "\n",
      "iter: 1 alpha: 0.3 starting points:[-0.5021926059012495, -0.5342527158461803]\n",
      "Failed to converge after 100000 iterations, function value 0.384422\n",
      "\n",
      "iter: 2 alpha: 0.05 starting points:[0.10848592319440979, -2.4168982856745216]\n",
      "x = 0.7011309377, -3.4622019832, F(x) = 0.31707947, |grad F| = 0.00000100 \n",
      "Converged after 5063 iterations, function value 0.317079\n",
      "\n",
      "iter: 2 alpha: 0.1 starting points:[0.10848592319440979, -2.4168982856745216]\n",
      "x = 0.7011411334, -3.4622551706, F(x) = 0.31707947, |grad F| = 0.00000100 \n",
      "Converged after 3362 iterations, function value 0.317079\n",
      "\n",
      "iter: 2 alpha: 0.3 starting points:[0.10848592319440979, -2.4168982856745216]\n",
      "Failed to converge after 100000 iterations, function value 0.384422\n",
      "\n",
      "iter: 3 alpha: 0.05 starting points:[0.41384371974756595, 0.5006712745900337]\n",
      "x = 0.7011309359, -3.4622019738, F(x) = 0.31707947, |grad F| = 0.00000100 \n",
      "Converged after 5605 iterations, function value 0.317079\n",
      "\n",
      "iter: 3 alpha: 0.1 starting points:[0.41384371974756595, 0.5006712745900337]\n",
      "x = 0.7011309404, -3.4622019973, F(x) = 0.31707947, |grad F| = 0.00000100 \n",
      "Converged after 2784 iterations, function value 0.317079\n",
      "\n",
      "iter: 3 alpha: 0.3 starting points:[0.41384371974756595, 0.5006712745900337]\n",
      "Failed to converge after 100000 iterations, function value 0.384422\n",
      "\n",
      "iter: 4 alpha: 0.05 starting points:[-0.3689276073834374, 0.6616118768116558]\n",
      "x = 0.7011309312, -3.4622019491, F(x) = 0.31707947, |grad F| = 0.00000100 \n",
      "Converged after 5606 iterations, function value 0.317079\n",
      "\n",
      "iter: 4 alpha: 0.1 starting points:[-0.3689276073834374, 0.6616118768116558]\n",
      "x = 0.7011309474, -3.4622020333, F(x) = 0.31707947, |grad F| = 0.00000100 \n",
      "Converged after 2790 iterations, function value 0.317079\n",
      "\n",
      "iter: 4 alpha: 0.3 starting points:[-0.3689276073834374, 0.6616118768116558]\n",
      "Failed to converge after 100000 iterations, function value 0.449638\n",
      "\n",
      "iter: 5 alpha: 0.05 starting points:[-1.9013729423357715, 2.7462059962802896]\n",
      "x = 0.7011309370, -3.4622019792, F(x) = 0.31707947, |grad F| = 0.00000100 \n",
      "Converged after 5759 iterations, function value 0.317079\n",
      "\n",
      "iter: 5 alpha: 0.1 starting points:[-1.9013729423357715, 2.7462059962802896]\n",
      "x = 0.7011309316, -3.4622019514, F(x) = 0.31707947, |grad F| = 0.00000100 \n",
      "Converged after 2863 iterations, function value 0.317079\n",
      "\n",
      "iter: 5 alpha: 0.3 starting points:[-1.9013729423357715, 2.7462059962802896]\n",
      "Failed to converge after 100000 iterations, function value 0.449638\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q5B - Steepest Descent w/ Constant Step Size\n",
    "tol = 1e-6;     # tolerance on norm of gradient\n",
    "MaxIter = 100000;\n",
    "alpha_lvl = [0.05, 0.1, 0.3]\n",
    "\n",
    "for i in range(1,5)\n",
    "    x0 = randn(2)\n",
    "    for alpha in alpha_lvl\n",
    "        print(\"iter: \",i ,\" alpha: \", alpha, \" starting points:\", x0, \"\\n\")\n",
    "        SteepestDescent(x0, alpha)\n",
    "        print(\"\\n\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "245cb907",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1 starting points: [0.1418047773644099, -0.16141338682428366]\n",
      "x = 0.7011329956, -3.4622130507, F(x) = 0.31707947, |grad F| = 0.00000098 \n",
      "Converged after 1487 iterations, function value 0.317079\n",
      "\n",
      "iter: 2 starting points: [1.0642505462225191, 1.4122750614559334]\n",
      "x = 0.7011392273, -3.4622455472, F(x) = 0.31707947, |grad F| = 0.00000099 \n",
      "Converged after 2151 iterations, function value 0.317079\n",
      "\n",
      "iter: 3 starting points: [-0.2985051515380857, -1.9002592599443566]\n",
      "x = 65.2681485795, -3.1899207188, F(x) = 3.70013409, |grad F| = 0.00000099 \n",
      "Converged after 30938 iterations, function value 3.700134\n",
      "\n",
      "iter: 4 starting points: [1.65756724301468, 0.03596665548738266]\n",
      "x = 0.7011331060, -3.4622129617, F(x) = 0.31707947, |grad F| = 0.00000099 \n",
      "Converged after 1337 iterations, function value 0.317079\n",
      "\n",
      "iter: 5 starting points: [0.035231997229772785, -0.541746713303703]\n",
      "x = 0.7011326591, -3.4622112815, F(x) = 0.31707947, |grad F| = 0.00000100 \n",
      "Converged after 1296 iterations, function value 0.317079\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5B - Steepest Descent w/ Armijo\n",
    "tol = 1e-6;     # tolerance on norm of gradient\n",
    "MaxIter = 100000;\n",
    "alpha_lvl = [0.05, 0.1, 0.3]\n",
    "\n",
    "for i in range(1,5)\n",
    "    x0 = randn(2)\n",
    "    print(\"iter: \", i, \" starting points: \", x0, \"\\n\")\n",
    "    SteepestDescentArmijo(x0)\n",
    "    print(\"\\n\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "1c64017f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1 , starting points: [0.43851347604982266, -0.20602953315044753]\n",
      "\n",
      "Converged after 15 iterations, F(x) = 3.700133\n",
      "x = [34.61213811697523 -3.1892367883780666]\n",
      "\n",
      "iter: 2 , starting points: [-0.22691254145630663, -0.09630099388813051]\n",
      "Caught an error: ArgumentError(\"matrix contains Infs or NaNs\")\n",
      "Skipping iteration 18\n",
      "Failed to converge after 18 iterations, function value 26.248986\n",
      "Failed to converge after 100000 iterations, function value 26.248986\n",
      "\n",
      "iter: 3 , starting points: [1.315219835985126, -0.4244819859753539]\n",
      "Caught an error: ArgumentError(\"matrix contains Infs or NaNs\")\n",
      "Skipping iteration 6\n",
      "Failed to converge after 6 iterations, function value 25.115568\n",
      "Failed to converge after 100000 iterations, function value 25.115568\n",
      "\n",
      "iter: 4 , starting points: [0.3749059125175739, -2.1177100043706467]\n",
      "\n",
      "Converged after 7 iterations, F(x) = 0.317079\n",
      "x = [0.7011360304332431 -3.4622285543058413]\n",
      "\n",
      "iter: 5 , starting points: [0.24238154004425347, 0.8144804389232957]\n",
      "\n",
      "Converged after 17 iterations, F(x) = 7.052310\n",
      "x = [33.58294133445634 115.87455952255627]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5C - Newton's Method\n",
    "tol = 1e-6;     # tolerance on norm of gradient\n",
    "MaxIterNewton = 1000;\n",
    "\n",
    "for i in range(1,5)\n",
    "    x0 = randn(2)\n",
    "    print(\"iter: \", i, \" , starting points: \", x0, \"\\n\")\n",
    "    Newtonv1(x0)\n",
    "    print(\"\\n\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "f9041022",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1 , starting points: [-1.418900641788563, -0.5311860376083344]\n",
      "\n",
      " Minimum eigenvalue: -1.2898189461900165\n",
      "\n",
      " Minimum eigenvalue: -0.3268018258116341\n",
      "\n",
      " Minimum eigenvalue: -0.09504796802586057\n",
      "\n",
      " Minimum eigenvalue: -0.06635112937675093\n",
      "\n",
      " Minimum eigenvalue: -0.02161228924128705\n",
      "\n",
      " Minimum eigenvalue: -0.003034336064654262\n",
      "Caught an error: ArgumentError(\"matrix contains Infs or NaNs\")\n",
      "Skipping iteration 7\n",
      "Failed to converge after 7 iterations, function value 22.232779\n",
      "Failed to converge after 100000 iterations, function value 22.232779\n",
      "\n",
      "iter: 2 , starting points: [1.0577820821166213, -1.0254683613218465]\n",
      "\n",
      " Minimum eigenvalue: -2.168112523170202\n",
      "\n",
      " Minimum eigenvalue: -0.6819522100212643\n",
      "\n",
      " Minimum eigenvalue: -0.22665142537604183\n",
      "\n",
      " Minimum eigenvalue: -0.004885564457333229\n",
      "\n",
      " Minimum eigenvalue: -0.0015655816631194477\n",
      "\n",
      " Minimum eigenvalue: -0.0004106545587024223\n",
      "\n",
      " Minimum eigenvalue: -0.0001063344163991526\n",
      "\n",
      " Minimum eigenvalue: -3.100200369507162e-5\n",
      "\n",
      " Minimum eigenvalue: -1.0331935517344723e-5\n",
      "\n",
      " Minimum eigenvalue: -3.6696225122149345e-6\n",
      "\n",
      " Minimum eigenvalue: -1.3330244020199646e-6\n",
      "\n",
      " Minimum eigenvalue: -4.881364825807567e-7\n",
      "Caught an error: ArgumentError(\"matrix contains Infs or NaNs\")\n",
      "Skipping iteration 13\n",
      "Failed to converge after 13 iterations, function value 2.617906\n",
      "Failed to converge after 100000 iterations, function value 2.617906\n",
      "\n",
      "iter: 3 , starting points: [0.7136652426234024, -1.275131009570029]\n",
      "\n",
      " Minimum eigenvalue: -2.279917953236124\n",
      "\n",
      " Minimum eigenvalue: -0.906004738126799\n",
      "\n",
      " Minimum eigenvalue: -0.23650084206700644\n",
      "\n",
      " Minimum eigenvalue: -0.03438530623985564\n",
      "\n",
      " Minimum eigenvalue: -0.013886234117719857\n",
      "\n",
      " Minimum eigenvalue: -0.0053964570584916684\n",
      "\n",
      " Minimum eigenvalue: -0.0021743016370537474\n",
      "\n",
      " Minimum eigenvalue: -0.0009044261300803115\n",
      "\n",
      " Minimum eigenvalue: -0.0003780766829980229\n",
      "Caught an error: ArgumentError(\"matrix contains Infs or NaNs\")\n",
      "Skipping iteration 10\n",
      "Failed to converge after 10 iterations, function value 2.105514\n",
      "Failed to converge after 100000 iterations, function value 2.105514\n",
      "\n",
      "iter: 4 , starting points: [-1.9953730684610687, 1.1776222081999261]\n",
      "\n",
      " Minimum eigenvalue: -0.22357240304654402\n",
      "\n",
      " Minimum eigenvalue: -0.05959449801690837\n",
      "\n",
      " Minimum eigenvalue: -0.013157479699304166\n",
      "\n",
      " Minimum eigenvalue: -0.04808230616523548\n",
      "\n",
      " Minimum eigenvalue: -0.01828376210127056\n",
      "\n",
      " Minimum eigenvalue: -0.006609736682301345\n",
      "\n",
      " Minimum eigenvalue: -0.002213844287815318\n",
      "\n",
      " Minimum eigenvalue: -0.0005576194463624456\n",
      "\n",
      " Minimum eigenvalue: -4.4916954419570226e-5\n",
      "\n",
      " Minimum eigenvalue: -7.12625529842821e-6\n",
      "\n",
      " Minimum eigenvalue: -2.1221684252215143e-6\n",
      "\n",
      " Minimum eigenvalue: -7.288467598667837e-7\n",
      "\n",
      " Minimum eigenvalue: -2.6170652632744994e-7\n",
      "\n",
      " Minimum eigenvalue: -9.543450933510624e-8\n",
      "\n",
      "Converged after 15 iterations, F(x) = 5.008503\n",
      "x = [37.80676403651109 52.31203777260204]\n",
      "\n",
      "iter: 5 , starting points: [0.07564373783988415, -0.956293115786866]\n",
      "\n",
      " Minimum eigenvalue: 2.019112358737125\n",
      "\n",
      " Minimum eigenvalue: 1.3664610325934528\n",
      "\n",
      " Minimum eigenvalue: 0.6756803424435152\n",
      "\n",
      " Minimum eigenvalue: 0.31081387077908695\n",
      "\n",
      " Minimum eigenvalue: 0.14077906697261067\n",
      "\n",
      " Minimum eigenvalue: 0.07000758836950349\n",
      "\n",
      " Minimum eigenvalue: 0.04413832046598559\n",
      "\n",
      " Minimum eigenvalue: 0.03747827101456014\n",
      "\n",
      " Minimum eigenvalue: 0.036866464394615495\n",
      "\n",
      "Converged after 10 iterations, F(x) = 0.317079\n",
      "x = [0.7011360355429714 -3.462228578044467]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5E - Newton's Method\n",
    "tol = 1e-6;     # tolerance on norm of gradient\n",
    "MaxIterNewton = 500;\n",
    "\n",
    "for i in range(1,5)\n",
    "    x0 = randn(2)\n",
    "    print(\"iter: \", i, \" , starting points: \", x0, \"\\n\")\n",
    "    Newtonv2(x0)\n",
    "    print(\"\\n\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "6974210d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1 , starting points: [2.124054980511592, 0.2940127394627716]\n",
      "x = 2.1240549805, 0.2940127395, F(x) = 3.60462024, |grad F| = 0.42642658, tau = 2.2678, alpha = 1.0000 \n",
      "x = 4.2237177453, -0.4086569006, F(x) = 3.61398912, |grad F| = 0.21584268, tau = 2.0725, alpha = 1.0000 \n",
      "x = 8.2875211997, -1.8158399297, F(x) = 3.62590276, |grad F| = 0.08778250, tau = 2.0566, alpha = 1.0000 \n",
      "x = 12.1326627071, -2.4690790052, F(x) = 3.67657905, |grad F| = 0.02854589, tau = 2.0311, alpha = 1.0000 \n",
      "x = 15.0589134116, -2.8496618235, F(x) = 3.69180514, |grad F| = 0.01000950, tau = 2.0176, alpha = 1.0000 \n",
      "x = 17.5827551673, -3.0836466585, F(x) = 3.69709590, |grad F| = 0.00354740, tau = 2.0124, alpha = 1.0000 \n",
      "x = 19.7877416579, -3.1761237932, F(x) = 3.69902094, |grad F| = 0.00125944, tau = 2.0106, alpha = 1.0000 \n",
      "x = 21.8208804737, -3.1890384465, F(x) = 3.69972556, |grad F| = 0.00045662, tau = 2.0101, alpha = 1.0000 \n",
      "x = 23.8239642029, -3.1892409007, F(x) = 3.69998394, |grad F| = 0.00016780, tau = 2.0100, alpha = 1.0000 \n",
      "x = 25.8249407774, -3.1892373463, F(x) = 3.70007887, |grad F| = 0.00006172, tau = 2.0100, alpha = 1.0000 \n",
      "x = 27.8253014401, -3.1892368637, F(x) = 3.70011378, |grad F| = 0.00002270, tau = 2.0100, alpha = 1.0000 \n",
      "x = 29.8254343115, -3.1892367985, F(x) = 3.70012662, |grad F| = 0.00000835, tau = 2.0100, alpha = 1.0000 \n",
      "x = 31.8254832179, -3.1892367897, F(x) = 3.70013134, |grad F| = 0.00000307, tau = 2.0100, alpha = 1.0000 \n",
      "x = 33.8255012131, -3.1892367885, F(x) = 3.70013308, |grad F| = 0.00000113, tau = 2.0100, alpha = 1.0000 \n",
      "\n",
      "Converged after 15 iterations, F(x) = 3.700134\n",
      "\n",
      " x = [35.825507833631015 -3.1892367883182873]\n",
      "\n",
      "iter: 2 , starting points: [-0.2043085956054338, -0.44433788005419667]\n",
      "x = -0.2043085956, -0.4443378801, F(x) = 16.06632067, |grad F| = 26.92893325, tau = 150.8176, alpha = 1.0000 \n",
      "x = 0.3274766859, -13.1871378715, F(x) = 13.89271674, |grad F| = 0.14659559, tau = 4.6051, alpha = 1.0000 \n",
      "x = 0.3270745567, -14.1823308811, F(x) = 13.89807936, |grad F| = 0.05388348, tau = 2.9653, alpha = 1.0000 \n",
      "x = 0.3269273834, -15.1805727071, F(x) = 13.90004820, |grad F| = 0.01981643, tau = 2.3615, alpha = 1.0000 \n",
      "x = 0.3268733437, -16.1799272826, F(x) = 13.90077197, |grad F| = 0.00728922, tau = 2.1393, alpha = 1.0000 \n",
      "x = 0.3268534774, -17.1796900293, F(x) = 13.90103815, |grad F| = 0.00268144, tau = 2.0576, alpha = 1.0000 \n",
      "x = 0.3268461709, -18.1796027737, F(x) = 13.90113607, |grad F| = 0.00098643, tau = 2.0275, alpha = 1.0000 \n",
      "x = 0.3268434832, -19.1795706775, F(x) = 13.90117209, |grad F| = 0.00036289, tau = 2.0164, alpha = 1.0000 \n",
      "x = 0.3268424945, -20.1795588705, F(x) = 13.90118534, |grad F| = 0.00013350, tau = 2.0124, alpha = 1.0000 \n",
      "x = 0.3268421308, -21.1795545270, F(x) = 13.90119021, |grad F| = 0.00004911, tau = 2.0109, alpha = 1.0000 \n",
      "x = 0.3268419970, -22.1795529291, F(x) = 13.90119200, |grad F| = 0.00001807, tau = 2.0103, alpha = 1.0000 \n",
      "x = 0.3268419478, -23.1795523413, F(x) = 13.90119266, |grad F| = 0.00000665, tau = 2.0101, alpha = 1.0000 \n",
      "x = 0.3268419297, -24.1795521250, F(x) = 13.90119291, |grad F| = 0.00000245, tau = 2.0100, alpha = 1.0000 \n",
      "\n",
      "Converged after 14 iterations, F(x) = 13.901193\n",
      "\n",
      " x = [0.3268419230209617 -25.17955204545821]\n",
      "\n",
      "iter: 3 , starting points: [0.19549125152143002, -0.6264545836067014]\n",
      "x = 0.1954912515, -0.6264545836, F(x) = 1.15353306, |grad F| = 4.03570886, tau = 0.3407, alpha = 1.0000 \n",
      "x = 0.2789109519, -1.2459299319, F(x) = 0.64758627, |grad F| = 1.99549691, tau = 1.2760, alpha = 1.0000 \n",
      "x = 0.3804707370, -1.8394294411, F(x) = 0.43794049, |grad F| = 0.90280222, tau = 1.7014, alpha = 1.0000 \n",
      "x = 0.4942526236, -2.4404750100, F(x) = 0.35144906, |grad F| = 0.37280917, tau = 1.8758, alpha = 1.0000 \n",
      "x = 0.6020886895, -2.9814616643, F(x) = 0.32298559, |grad F| = 0.12946433, tau = 1.9434, alpha = 1.0000 \n",
      "x = 0.6747690343, -3.3359389964, F(x) = 0.31743401, |grad F| = 0.02991700, tau = 1.9670, alpha = 1.0000 \n",
      "x = 0.6990409207, -3.4522955920, F(x) = 0.31708161, |grad F| = 0.00241965, tau = 1.9727, alpha = 1.0000 \n",
      "x = 0.7011222182, -3.4621635301, F(x) = 0.31707948, |grad F| = 0.00001693, tau = 1.9731, alpha = 1.0000 \n",
      "\n",
      "Converged after 9 iterations, F(x) = 0.317079\n",
      "\n",
      " x = [0.7011360371085875 -3.462228585362292]\n",
      "\n",
      "iter: 4 , starting points: [0.1372031414169958, -0.8248757155352252]\n",
      "x = 0.1372031414, -0.8248757155, F(x) = 1.66166330, |grad F| = 16.83045102, tau = 0.1552, alpha = 1.0000 \n",
      "x = 0.2213896987, -1.1626878902, F(x) = 0.87161482, |grad F| = 6.14819168, tau = 1.0292, alpha = 1.0000 \n",
      "x = 0.3145216847, -1.5974281723, F(x) = 0.54333957, |grad F| = 2.37773994, tau = 1.5378, alpha = 1.0000 \n",
      "x = 0.4220644509, -2.1181393318, F(x) = 0.39535229, |grad F| = 0.90800662, tau = 1.7953, alpha = 1.0000 \n",
      "x = 0.5361680822, -2.6714360876, F(x) = 0.33649774, |grad F| = 0.32140391, tau = 1.9099, alpha = 1.0000 \n",
      "x = 0.6343172939, -3.1439568752, F(x) = 0.31957633, |grad F| = 0.09286985, tau = 1.9554, alpha = 1.0000 \n",
      "x = 0.6884876132, -3.4023956260, F(x) = 0.31715897, |grad F| = 0.01546400, tau = 1.9703, alpha = 1.0000 \n",
      "x = 0.7006407238, -3.4598992744, F(x) = 0.31707959, |grad F| = 0.00061359, tau = 1.9730, alpha = 1.0000 \n",
      "\n",
      "Converged after 9 iterations, F(x) = 0.317079\n",
      "\n",
      " x = [0.7011352604608369 -3.462224947603775]\n",
      "\n",
      "iter: 5 , starting points: [-0.34776888120731847, 0.04544498609443826]\n",
      "x = -0.3477688812, 0.0454449861, F(x) = 19.60885948, |grad F| = 12.76588361, tau = 73.4314, alpha = 1.0000 \n",
      "x = -0.5178930331, 2.7084844290, F(x) = 23.03974629, |grad F| = 8.25221040, tau = 53.3261, alpha = 1.0000 \n",
      "x = -0.7649013194, 3.8651498629, F(x) = 24.31219062, |grad F| = 3.56799750, tau = 22.3468, alpha = 1.0000 \n",
      "x = -1.1157120131, 5.5143210488, F(x) = 25.09567826, |grad F| = 1.55612545, tau = 10.6202, alpha = 1.0000 \n",
      "x = -1.6118431685, 7.8228768630, F(x) = 25.57578047, |grad F| = 0.67741831, tau = 5.7301, alpha = 1.0000 \n",
      "x = -2.3065370319, 11.0280049652, F(x) = 25.86596192, |grad F| = 0.29328317, tau = 3.5558, alpha = 1.0000 \n",
      "x = -3.2798025439, 15.5099332288, F(x) = 26.04126966, |grad F| = 0.12817124, tau = 2.5170, alpha = 1.0000 \n",
      "x = -4.6366439251, 21.6752593912, F(x) = 26.14684324, |grad F| = 0.05621929, tau = 2.1395, alpha = 1.0000 \n",
      "x = -6.6008237105, 31.1362387089, F(x) = 26.21525428, |grad F| = 0.01441074, tau = 2.4719, alpha = 1.0000 \n",
      "x = -10.7101304288, 51.2813013494, F(x) = 26.28175543, |grad F| = 0.07518724, tau = 2.7024, alpha = 1.0000 \n",
      "x = -16.1322468952, 76.7735564768, F(x) = 26.32071648, |grad F| = 0.01369385, tau = 2.3192, alpha = 1.0000 \n",
      "x = -20.4539148470, 97.2888578054, F(x) = 26.33023517, |grad F| = 0.00475922, tau = 2.1267, alpha = 1.0000 \n",
      "Caught an error: ArgumentError(\"matrix contains Infs or NaNs\")\n",
      "Skipping iteration 13\n",
      "Failed to converge after 13 iterations, function value 26.333476\n",
      "Failed to converge after 100000 iterations, function value 26.333476\n",
      "\n",
      "iter: 6 , starting points: [-0.674545330863388, 0.1455584064544368]\n",
      "x = -0.6745453309, 0.1455584065, F(x) = 21.77571944, |grad F| = 3.01402472, tau = 12.2717, alpha = 1.0000 \n",
      "x = 11.4564346021, 102.3830061157, F(x) = 12.07466909, |grad F| = 0.50832319, tau = 2.0100, alpha = 1.0000 \n",
      "x = 14.9963059172, 133.4789174040, F(x) = 12.05650022, |grad F| = 0.13698105, tau = 2.0100, alpha = 1.0000 \n",
      "x = 17.8467126091, 158.7129462734, F(x) = 12.05479975, |grad F| = 0.03882711, tau = 2.0100, alpha = 1.0000 \n",
      "x = 20.3063640821, 180.6223100544, F(x) = 12.05490254, |grad F| = 0.00896583, tau = 2.0100, alpha = 1.0000 \n",
      "x = 22.4532980679, 199.8721503171, F(x) = 12.05507835, |grad F| = 0.00149831, tau = 2.0100, alpha = 1.0000 \n",
      "x = 24.4675684385, 217.9937876546, F(x) = 12.05515245, |grad F| = 0.00037378, tau = 2.0100, alpha = 1.0000 \n",
      "x = 26.4679306327, 235.9969915782, F(x) = 12.05517977, |grad F| = 0.00013603, tau = 2.0100, alpha = 1.0000 \n",
      "x = 28.4680227573, 253.9978207185, F(x) = 12.05518982, |grad F| = 0.00005004, tau = 2.0100, alpha = 1.0000 \n",
      "x = 30.4680566544, 271.9981257952, F(x) = 12.05519352, |grad F| = 0.00001841, tau = 2.0100, alpha = 1.0000 \n",
      "x = 32.4680691246, 289.9982380275, F(x) = 12.05519488, |grad F| = 0.00000677, tau = 2.0100, alpha = 1.0000 \n",
      "x = 34.4680737138, 307.9982793305, F(x) = 12.05519538, |grad F| = 0.00000249, tau = 2.0100, alpha = 1.0000 \n",
      "\n",
      "Converged after 13 iterations, F(x) = 12.055196\n",
      "\n",
      " x = [36.46807539946948 325.9982945017757]\n",
      "\n",
      "iter: 7 , starting points: [-0.23389081051113114, -0.24020468309105933]\n",
      "x = -0.2338908105, -0.2402046831, F(x) = 17.12667456, |grad F| = 23.97492406, tau = 142.7988, alpha = 1.0000 \n",
      "x = -0.6482536519, 6.7470570541, F(x) = 21.19715087, |grad F| = 16.05172618, tau = 59.5989, alpha = 1.0000 \n",
      "x = -2.8454100276, 44.5184713862, F(x) = 17.53000918, |grad F| = 6.76508278, tau = 2.6717, alpha = 1.0000 \n",
      "x = -5.8863319065, 93.0291827882, F(x) = 17.49901030, |grad F| = 3.41213724, tau = 8.8269, alpha = 1.0000 \n",
      "x = -12.2142503580, 193.1404969827, F(x) = 17.62617216, |grad F| = 1.43909494, tau = 17.3456, alpha = 1.0000 \n",
      "x = -17.0360213031, 268.9450012753, F(x) = 17.69445785, |grad F| = 0.45945108, tau = 8.2563, alpha = 1.0000 \n",
      "x = -21.1502778877, 333.6933429001, F(x) = 17.71234530, |grad F| = 0.16420441, tau = 4.3610, alpha = 1.0000 \n",
      "x = -25.1800953798, 397.1440486457, F(x) = 17.71842352, |grad F| = 0.05985151, tau = 2.8814, alpha = 1.0000 \n",
      "x = -29.1895952034, 460.2866777675, F(x) = 17.72060137, |grad F| = 0.02194680, tau = 2.3314, alpha = 1.0000 \n",
      "x = -33.1928968165, 523.3360998351, F(x) = 17.72139507, |grad F| = 0.00806430, tau = 2.1284, alpha = 1.0000 \n",
      "x = -37.1940854969, 586.3538721915, F(x) = 17.72168606, |grad F| = 0.00296542, tau = 2.0536, alpha = 1.0000 \n",
      "x = -41.1945192912, 649.3603550832, F(x) = 17.72179297, |grad F| = 0.00109074, tau = 2.0260, alpha = 1.0000 \n",
      "x = -45.1946784024, 712.3627325444, F(x) = 17.72183229, |grad F| = 0.00040124, tau = 2.0159, alpha = 1.0000 \n",
      "x = -49.1947368722, 775.3636061537, F(x) = 17.72184675, |grad F| = 0.00014760, tau = 2.0122, alpha = 1.0000 \n",
      "x = -53.1947583734, 838.3639274002, F(x) = 17.72185207, |grad F| = 0.00005430, tau = 2.0108, alpha = 1.0000 \n",
      "x = -57.1947662821, 901.3640455617, F(x) = 17.72185402, |grad F| = 0.00001998, tau = 2.0103, alpha = 1.0000 \n",
      "x = -61.1947691913, 964.3640890285, F(x) = 17.72185474, |grad F| = 0.00000735, tau = 2.0101, alpha = 1.0000 \n",
      "x = -65.1947702616, 1027.3641050189, F(x) = 17.72185501, |grad F| = 0.00000270, tau = 2.0100, alpha = 1.0000 \n",
      "\n",
      "Converged after 19 iterations, F(x) = 17.721855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " x = [-69.19477065531922 1090.3641109011564]\n",
      "\n",
      "iter: 8 , starting points: [0.7227656263280802, 0.5353668201782649]\n",
      "x = 0.7227656263, 0.5353668202, F(x) = 3.35704300, |grad F| = 1.30043216, tau = 3.8375, alpha = 1.0000 \n",
      "x = 1.5331691317, 0.5072290913, F(x) = 3.60759338, |grad F| = 0.59345391, tau = 2.5059, alpha = 1.0000 \n",
      "x = 3.0079908814, 0.1232874089, F(x) = 3.64662386, |grad F| = 0.29967374, tau = 2.1284, alpha = 1.0000 \n",
      "x = 6.1414235249, -0.8981965365, F(x) = 3.63727170, |grad F| = 0.13652411, tau = 2.0467, alpha = 1.0000 \n",
      "x = 10.2689583527, -1.8855800689, F(x) = 3.67040266, |grad F| = 0.04543767, tau = 2.0316, alpha = 1.0000 \n",
      "x = 13.5349590315, -2.4111120229, F(x) = 3.68943422, |grad F| = 0.01567881, tau = 2.0181, alpha = 1.0000 \n",
      "x = 16.3842324463, -2.7984008017, F(x) = 3.69612902, |grad F| = 0.00550193, tau = 2.0127, alpha = 1.0000 \n",
      "x = 18.9226546098, -3.0541561091, F(x) = 3.69863123, |grad F| = 0.00190542, tau = 2.0108, alpha = 1.0000 \n",
      "x = 21.1605341951, -3.1681612385, F(x) = 3.69957794, |grad F| = 0.00064965, tau = 2.0102, alpha = 1.0000 \n",
      "x = 23.2051538599, -3.1886517765, F(x) = 3.69992963, |grad F| = 0.00022923, tau = 2.0101, alpha = 1.0000 \n",
      "x = 25.2076645344, -3.1892373864, F(x) = 3.70005891, |grad F| = 0.00008403, tau = 2.0100, alpha = 1.0000 \n",
      "x = 27.2081560196, -3.1892369281, F(x) = 3.70010644, |grad F| = 0.00003091, tau = 2.0100, alpha = 1.0000 \n",
      "x = 29.2083368599, -3.1892368072, F(x) = 3.70012392, |grad F| = 0.00001137, tau = 2.0100, alpha = 1.0000 \n",
      "x = 31.2084034352, -3.1892367909, F(x) = 3.70013035, |grad F| = 0.00000418, tau = 2.0100, alpha = 1.0000 \n",
      "x = 33.2084279333, -3.1892367886, F(x) = 3.70013271, |grad F| = 0.00000154, tau = 2.0100, alpha = 1.0000 \n",
      "\n",
      "Converged after 16 iterations, F(x) = 3.700134\n",
      "\n",
      " x = [35.20843694653219 -3.1892367883398625]\n",
      "\n",
      "iter: 9 , starting points: [0.6680575885560879, 0.8971531571096966]\n",
      "x = 0.6680575886, 0.8971531571, F(x) = 3.78705592, |grad F| = 1.29428849, tau = 3.3573, alpha = 1.0000 \n",
      "x = 1.4244913362, 1.2285581195, F(x) = 4.03006417, |grad F| = 0.63504124, tau = 2.3727, alpha = 1.0000 \n",
      "x = 2.8121247378, 1.5235598032, F(x) = 4.06058762, |grad F| = 0.31500682, tau = 2.1013, alpha = 1.0000 \n",
      "x = 5.6122359349, 2.1455229278, F(x) = 4.09529251, |grad F| = 0.15084697, tau = 2.0363, alpha = 1.0000 \n",
      "x = 9.7416321774, 3.0641186471, F(x) = 4.12082285, |grad F| = 0.04954844, tau = 2.0276, alpha = 1.0000 \n",
      "x = 12.9755164961, 4.1829386635, F(x) = 4.14585445, |grad F| = 0.01675880, tau = 2.0167, alpha = 1.0000 \n",
      "x = 15.6720868405, 5.2256520067, F(x) = 4.15459747, |grad F| = 0.00576409, tau = 2.0120, alpha = 1.0000 \n",
      "x = 17.9948338135, 6.2410938789, F(x) = 4.15780399, |grad F| = 0.00196283, tau = 2.0105, alpha = 1.0000 \n",
      "x = 20.0699133720, 7.2467622403, F(x) = 4.15898457, |grad F| = 0.00068642, tau = 2.0102, alpha = 1.0000 \n",
      "x = 22.0766784591, 8.2488454289, F(x) = 4.15941719, |grad F| = 0.00025090, tau = 2.0101, alpha = 1.0000 \n",
      "x = 24.0781938599, 9.2496100725, F(x) = 4.15957603, |grad F| = 0.00009228, tau = 2.0100, alpha = 1.0000 \n",
      "x = 26.0787535289, 10.2498910764, F(x) = 4.15963443, |grad F| = 0.00003395, tau = 2.0100, alpha = 1.0000 \n",
      "x = 28.0789598850, 11.2499944123, F(x) = 4.15965590, |grad F| = 0.00001249, tau = 2.0100, alpha = 1.0000 \n",
      "x = 30.0790358619, 12.2500324221, F(x) = 4.15966380, |grad F| = 0.00000459, tau = 2.0100, alpha = 1.0000 \n",
      "x = 32.0790638207, 13.2500464044, F(x) = 4.15966671, |grad F| = 0.00000169, tau = 2.0100, alpha = 1.0000 \n",
      "\n",
      "Converged after 16 iterations, F(x) = 4.159668\n",
      "\n",
      " x = [34.0790741073108 14.25005154812156]\n",
      "\n",
      "iter: 10 , starting points: [1.9168188694607529, 0.5685623158477132]\n",
      "x = 1.9168188695, 0.5685623158, F(x) = 3.69436779, |grad F| = 0.46272431, tau = 2.2987, alpha = 1.0000 \n",
      "x = 3.7919440689, 0.2016909938, F(x) = 3.72053484, |grad F| = 0.23791885, tau = 2.0777, alpha = 1.0000 \n",
      "x = 7.9023309315, -0.8866028657, F(x) = 3.69071375, |grad F| = 0.09188517, tau = 2.0248, alpha = 1.0000 \n",
      "x = 11.7132111838, -1.7355173433, F(x) = 3.69155735, |grad F| = 0.02919339, tau = 2.0159, alpha = 1.0000 \n",
      "x = 14.8829589710, -2.2791286753, F(x) = 3.69631199, |grad F| = 0.01002182, tau = 2.0121, alpha = 1.0000 \n",
      "x = 17.7516332971, -2.6975471960, F(x) = 3.69850396, |grad F| = 0.00342701, tau = 2.0107, alpha = 1.0000 \n",
      "x = 20.3547477718, -2.9931418721, F(x) = 3.69946865, |grad F| = 0.00112437, tau = 2.0102, alpha = 1.0000 \n",
      "x = 22.6682642330, -3.1476757570, F(x) = 3.69987872, |grad F| = 0.00034482, tau = 2.0101, alpha = 1.0000 \n",
      "x = 24.7485475468, -3.1869869892, F(x) = 3.70003973, |grad F| = 0.00010840, tau = 2.0100, alpha = 1.0000 \n",
      "x = 26.7536508447, -3.1892301012, F(x) = 3.70009938, |grad F| = 0.00003880, tau = 2.0100, alpha = 1.0000 \n",
      "x = 28.7538915620, -3.1892368181, F(x) = 3.70012132, |grad F| = 0.00001427, tau = 2.0100, alpha = 1.0000 \n",
      "x = 30.7539751112, -3.1892367923, F(x) = 3.70012939, |grad F| = 0.00000525, tau = 2.0100, alpha = 1.0000 \n",
      "x = 32.7540058574, -3.1892367888, F(x) = 3.70013236, |grad F| = 0.00000193, tau = 2.0100, alpha = 1.0000 \n",
      "\n",
      "Converged after 14 iterations, F(x) = 3.700133\n",
      "\n",
      " x = [34.75401716969523 -3.189236788366816]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5F - Modified Newton\n",
    "tol = 1e-6;     # tolerance on norm of gradient\n",
    "MaxIterNewton = 500;\n",
    "\n",
    "for i in range(1,10)\n",
    "    x0 = randn(2)\n",
    "    print(\"iter: \", i, \" , starting points: \", x0, \"\\n\")\n",
    "    NewtonModified(x0)\n",
    "    print(\"\\n\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "63c48c6b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1 , starting points: [0.2911877849718705, 0.3984132240326721]\n",
      "Failed to converge after 100000 iterations, function value 2.861459\n",
      "\n",
      "iter: 2 , starting points: [1.4942031797891338, -2.203932541074327]\n",
      "Failed to converge after 100000 iterations, function value 2.112521\n",
      "\n",
      "iter: 3 , starting points: [-3.1166446400927725, -0.8890603580657512]\n",
      "Failed to converge after 100000 iterations, function value 22.631540\n",
      "\n",
      "iter: 4 , starting points: [-0.5201770617612161, 0.6760163673003925]\n",
      "Failed to converge after 100000 iterations, function value 21.857231\n",
      "\n",
      "iter: 5 , starting points: [0.2584889160366705, -0.8769875010146798]\n",
      "\n",
      "Converged after 8 iterations, F(x) = 0.317079\n",
      "\n",
      " x = [0.701135994405086 -3.4622283845968242]\n",
      "\n",
      "iter: 6 , starting points: [1.2972833409154232, 0.07386455113262047]\n",
      "Failed to converge after 100000 iterations, function value 3.269707\n",
      "\n",
      "iter: 7 , starting points: [0.05173491794923106, -0.9817526115437116]\n",
      "\n",
      "Converged after 10 iterations, F(x) = 0.317079\n",
      "\n",
      " x = [0.7011360376930422 -3.4622285881016426]\n",
      "\n",
      "iter: 8 , starting points: [-0.23878419694027808, -1.1401201938326297]\n",
      "Failed to converge after 100000 iterations, function value 13.831277\n",
      "\n",
      "iter: 9 , starting points: [-0.5660974155282904, -0.9600919241144128]\n",
      "Failed to converge after 100000 iterations, function value 17.468695\n",
      "\n",
      "iter: 10 , starting points: [-0.5871713689331672, -1.4147341659654775]\n",
      "Failed to converge after 100000 iterations, function value 18.046322\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5G - Modified Newton with BackTracking\n",
    "tol = 1e-6;     # tolerance on norm of gradient\n",
    "MaxIterNewton = 500;\n",
    "\n",
    "for i in range(1,10)\n",
    "    x0 = randn(2)\n",
    "    print(\"iter: \", i, \" , starting points: \", x0, \"\\n\")\n",
    "    NewtonModified_Backtrack(x0)\n",
    "    print(\"\\n\")\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
